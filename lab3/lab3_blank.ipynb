{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9199231",
   "metadata": {},
   "source": [
    "**Лабораторный практикум по курсу «Распознавание диктора», Университет ИТМО, 2021**\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dd7db",
   "metadata": {},
   "source": [
    "**Лабораторная работа №3. Построение дикторских моделей и их сравнение**\n",
    "\n",
    "**Цель работы:** изучение процедуры построения дикторских моделей с использованием глубоких нейросетевых архитектур.\n",
    "\n",
    "**Краткое описание:** в рамках настоящей лабораторной работы предлагается изучить и реализовать схему построения дикторских моделей с использованием глубокой нейросетевой архитектуры, построенной на основе ResNet-блоков. Процедуры обучения и тестирования предлагается рассмотреть по отношению к задаче идентификации на закрытом множестве, то есть для ситуации, когда дикторские классы являются строго заданными. Тестирование полученной системы предполагает использование доли правильных ответов (accuracy) в качестве целевой метрики оценки качества.\n",
    "\n",
    "**Данные:** в качестве данных для выполнения лабораторной работы предлагается использовать базу [VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html).\n",
    "\n",
    "**Содержание лабораторной работы**\n",
    "\n",
    "1. Подготовка данных для обучения и тестирования блока построения дикторских моделей.\t\t\t\t\t\t\t\n",
    "\n",
    "2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных.\n",
    "\n",
    "3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных.\n",
    "\n",
    "4. Тестированное блока построения дикторских моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f862cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# IPython extension to reload modules before executing user code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import of modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import download_dataset, concatenate, extract_dataset, part_extract, download_protocol, split_musan\n",
    "from exercises_blank import train_dataset_loader, test_dataset_loader, ResNet, MainModel, train_network, test_network\n",
    "from ResNetBlocks import BasicBlock\n",
    "from LossFunction import AAMSoftmaxLoss, AnglProto\n",
    "from Optimizer import SGDOptimizer\n",
    "from Scheduler import OneCycleLRScheduler\n",
    "from load_save_pth import saveParameters, loadParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3baf71",
   "metadata": {},
   "source": [
    "**1. Подготовка данных для обучения и тестирования детектора речевой активности**\n",
    "\n",
    "В ходе выполнения лабораторной работы необходимы данные для выполнения процедуры обучения и процедуры тестирования нейросетевого блока генерации дикторских моделей. Возьмём в качестве этих данных звукозаписи, сохраненные в формат *wav*, из корпуса [VoxCeleb1 dev set](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html). Данный корпус содержит 148,642 звукозаписи (частота дискретизации равна 16кГц) для 1,211 дикторов женского и мужского пола, разговаривающих преимущественно на английском языке.\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить загрузку и распаковку звуковых wav-файлов из корпуса VoxCeleb1 dev set.\n",
    "\n",
    "![Рисунок 1](https://analyticsindiamag.com/wp-content/uploads/2020/12/image.png \"VoxCeleb. Крупномасштабная аудиовизуальная база данных человеческой речи.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd7ecdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download VoxCeleb1 (test set)\n",
    "# with open('../data/lists/datasets.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# download_dataset(lines, user='voxceleb1902', password='nx0bl2v2', save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0fd3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate archives for VoxCeleb1 dev set\n",
    "# with open('../data/lists/concat_arch.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# concatenate(lines, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ed3a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract VoxCeleb1 dev set\n",
    "# extract_dataset(save_path='../data/voxceleb1_dev', fname='../data/vox1_dev_wav.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "875ec4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download VoxCeleb1 identification protocol\n",
    "# with open('../data/lists/protocols.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# download_protocol(lines, save_path='../data/voxceleb1_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6aee",
   "metadata": {},
   "source": [
    "**2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных**\n",
    "\n",
    "Построение современных дикторских моделей, как правило, выполняется с использованием нейросетевых архитектур, многие из которых позаимствованы из области обработки цифровых изображений. Одними из наиболее распространенных нейросетевых архитектур, используемыми для построения дикторских моделей, являются [ResNet-подобные архитектуры](https://arxiv.org/pdf/1512.03385.pdf). В рамках настоящего пункта предлагается выполнить адаптацию нейросетевой архитектуры ResNet34 для решения задачи генерации дикторских моделей (дикторских эмбеддингов). *Дикторский эмбеддинг* – это высокоуровневый вектор-признаков, состоящий, например, из 128, 256 и т.п. значений, содержащий особенности голоса конкретного человека. При решении задачи распознавания диктора можно выделить эталонные и тестовые дикторские эмбеддинги. *Эталонные эмбеддинги* формируются на этапе регистрации дикторской модели определённого человека и находятся в некотором хранилище данных. *Тестовые эмбеддинги* формируются на этапе непосредственного использования системы голосовой биометрии на практике, когда некоторый пользователь пытается получить доступ к соответствующим ресурсам. Система голосовой биометрии сравнивает по определённой метрике эталонные и тестовые эмбеддинги, формируя оценку сравнения, которая, после её обработки блоком принятия решения, позволяет сделать вывод о том, эмбеддинги одинаковых или разных дикторов сравниваются между собой.\n",
    "\n",
    "Адаптация различных нейросетевых архитектур из обработки изображений к решению задачи построения дикторских моделей является непростой задачей. Возьмём за основу готовое решение, предложенной в рамках [следующей публикации](https://arxiv.org/pdf/2002.06033.pdf) и адаптируем его применительно к выполнению настоящей лабораторной работы.\n",
    "\n",
    "Необходимо отметить, что построение дикторских моделей, как правило, требует наличия *акустических признаков*, вычисленных для звукозаписей тренировочной, валидационной и тестовой баз данных. В качестве примера подобных признаков в рамках настоящей лабораторной работы воспользуемся *логарифмами энергий на выходе мел-банка фильтров*. Важно отметить, что акустические признаки подвергаются некоторым процедурам предобработки перед их непосредственной передачей в блок построения дикторских моделей. В качестве этих процедур можно выделить: нормализация и масштабирование признаков, сохранение только речевых фреймов на основе разметки детектора речевой активности и т.п.\n",
    "\n",
    "После того, как акустические признаки подготовлены, они могут быть переданы на блок построения дикторских моделей. Как правило, структура современных дикторских моделей соответствует структуре [x-векторных архитектур](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf). Эти архитектуры состоят из четырёх ключевых элементов: \n",
    "\n",
    "1. **Фреймовый уровень.** Предназначен для формирования локальных представлений голоса конкретного человека. На этом уровне как раз и применяются нейросетевые архитектуры на базе свёрточных нейронных сетей, например, ResNet, позволяющих с использованием каскадной схемы из множества фильтров с локальной маской захватить некоторый локальный контекст шаблона голоса человека. Выходом фреймового уровня является набор высокоуровневых представлений (карт-признаков), содержащих локальные особенности голоса человека.\n",
    "\n",
    "2. **Уровень статистического пулинга** позволяет сформировать промежуточный вектор-признаков, фиксированной длины, которая является одинаковой для звукозаписи любой длительности. В ходе работы блока статистического пулинга происходит удаление временной размерности, присутствующей в картах-признаков. Это достигается путём выполнения процедуры усреднения карт-признаков вдоль оси времени. Выходом уровня статистического пулинга являются вектор среднего и вектор среднеквадратического отклонения, вычисленные на основе карт-признаков. Эти вектора конкатенируются и передаются для дальнейшей обработки на сегментом уровне.\n",
    "\n",
    "3. **Сегментный уровень.** Предназначен для трансформации промежуточного вектора, как правило, высокой размерности, в компактный вектор-признаков, представляющий собой дикторский эмбеддинг. Необходимо отметить, что на сегментном уровне расположены один или несколько полносвязных нейросетевых слоёв, а обработка данных выполняется по отношению ко всей звукозаписи, а не только к некоторому её локальному контексту, как на фреймовом уровне.\n",
    "\n",
    "4. **Уровень выходного слоя.** Представляет полносвязный слой с softmax-функциями активации. Количество активаций равно числу дикторов в тренирочной выборке. На вход выходноя слоя подаётся дикторский эмбеддинг, а на выходе – формируется набор апостериорных вероятностей, определяющих принадлежность эмбеддинга к одному из дикторских классов в тренировочной выборке. Необходимо отметить, что, как правило, в современных нейросетевых системах построения дикторских моделей выходной используется только на этапе обучения параметров и на этапе тестирования не используется (на этапе тестирования используются только три первых уровня архитектуры).\n",
    "\n",
    "Обучение модели генерации дикторских эмбеддингов выполняется путём решения задачи *классификации* или, выражаясь терминами из области биометрии, *идентификации на закрытом множестве* (количество дикторских меток является строго фиксированным). В качестве используемой стоимостной функции выступает *категориальная кросс-энтропия*. Обучение выполняется с помощью мини-батчей, содержащих короткие фрагменты карт акустических признаков (длительностью несколько секунд) различных дикторов из тренировочной базы данных. Обучение на коротких фрагментов позволяет избежать сильного переобучения нейросетевой модели. При выполнении процедуры обучения требуется подобрать набор гиперпараметров, выбрать обучения и метод численной оптимизации.\n",
    "\n",
    "Для успешного выполнения настоящего пункта необходимо сделать следующее:\n",
    "\n",
    "1. Сгенерировать списки тренировочных, валидационных и тестовых данных на основе идентификационного протокола базы VoxCeleb1, содержащегося в файле **../data/voxceleb1_test/iden_split.txt**. При генерации списков требуется исключить из них звукозаписи дикторов, которые входят в базу [VoxCeleb1 test set](https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip). Это позволит выполнить тестирования обученных блоков генерации дикторских моделей на протоколе [VoxCeleb1-O cleaned](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt), который составлен по отношению к данным из VoxCeleb1 test set, в лабораторной работе №4.\n",
    "\n",
    "2. Инициализировать обучаемую дикторскую модель, выбрав любой возможный вариант её архитектуры, предлагаемый в рамках лабораторной работы. При реализации блока статистического пулинга предлагается выбрать либо его классический вариант, предложенный в [следующей работе](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), либо его более продвинутую версию основанную на использовании [механизмов внимания](https://arxiv.org/pdf/1803.10963.pdf). Использование последней версии статистического пулинга позволяет реализовать детектор речевой активности прямо внутри блока построения дикторских моделей.\n",
    "\n",
    "3. Инициализировать загрузчики тренировочной и валидационной выборки.\n",
    "\n",
    "4. Инициализировать оптимизатор и планировщик для выполнения процедуры обучения.\n",
    "\n",
    "5. Описать процедуру валидации/тестирования блока построения дикторских моделей.\n",
    "\n",
    "6. Описать процедуру обучения и запустить её, контролируя значения стоимостной функции и доли правильных ответов на тренировочном множестве, а также долю правильных ответов на валидационном множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3380f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "\n",
    "# Acoustic features\n",
    "n_mels            = 40                                   # number of mel filters in bank filters\n",
    "log_input         = True                                 # logarithm of features by level\n",
    "\n",
    "# Neural network archtecture\n",
    "layers            = [3, 4, 6, 3]                         # number of ResNet blocks in different level of frame level\n",
    "activation        = nn.ReLU                              # activation function used in ResNet blocks\n",
    "num_filters       = [32, 64, 128, 256]                   # number of filters of ResNet blocks in different level of frame level\n",
    "encoder_type      = 'ASP'                                 # type of statistic pooling layer ('SP'  – classical statistic pooling \n",
    "                                                         # layer and 'ASP' – attentive statistic pooling)\n",
    "nOut              = 512                                  # embedding size\n",
    "\n",
    "# Loss function for angular losses\n",
    "margin            = 0.35                                 # margin parameter\n",
    "scale             = 32.0                                 # scale parameter\n",
    "\n",
    "# Train dataloader\n",
    "max_frames_train  = 200                                  # number of frame to train\n",
    "train_path        = '/mnt/storage/work_dir/databases/voxceleb1_dev/wav'           # path to train wav files\n",
    "batch_size_train  = 128                                  # batch size to train\n",
    "pin_memory        = False                                # pin memory\n",
    "num_workers_train = 1                                    # number of workers to train\n",
    "shuffle           = False                                 # shuffling of training examples\n",
    "\n",
    "# Validation dataloader\n",
    "max_frames_val    = 200                                 # number of frame to validate\n",
    "val_path          = '/mnt/storage/work_dir/databases/voxceleb1_dev/wav'           # path to val wav files\n",
    "batch_size_val    = 128                                  # batch size to validate\n",
    "num_workers_val   = 1                                   # number of workers to validate\n",
    "\n",
    "# Test dataloader\n",
    "max_frames_test   = 200                                 # number of frame to test\n",
    "test_path         = '/mnt/storage/work_dir/databases/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_test   = 128                                  # batch size to test\n",
    "num_workers_test  = 1                                    # number of workers to test\n",
    "\n",
    "# Optimizer\n",
    "lr                = 2.5                                  # learning rate value\n",
    "weight_decay      = 0                                    # weight decay value\n",
    "\n",
    "# Scheduler\n",
    "val_interval      = 5                                    # frequency of validation step\n",
    "max_epoch         = 40                                   # number of epoches\n",
    "\n",
    "# Augmentation\n",
    "musan_path        = '/mnt/storage/work_dir/databases/musan_split'                                # path to splitted SLR17 dataset\n",
    "rir_path          = '/mnt/storage/work_dir/databases/RIRS_NOISES/simulated_rirs' # path to SLR28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select hyperparameters\n",
    "\n",
    "# # Acoustic features\n",
    "# n_mels            = 40                                   # number of mel filters in bank filters\n",
    "# log_input         = True                                 # logarithm of features by level\n",
    "\n",
    "# # Neural network archtecture\n",
    "# layers            = [3, 4, 6, 3]                         # number of ResNet blocks in different level of frame level\n",
    "# activation        = nn.ReLU                              # activation function used in ResNet blocks\n",
    "# num_filters       = [32, 64, 128, 256]                   # number of filters of ResNet blocks in different level of frame level\n",
    "# encoder_type      = 'SP'                                 # type of statistic pooling layer ('SP'  – classical statistic pooling \n",
    "#                                                          # layer and 'ASP' – attentive statistic pooling)\n",
    "# nOut              = 512                                  # embedding size\n",
    "\n",
    "# # Loss function for angular losses\n",
    "# margin            = 0.35                                 # margin parameter\n",
    "# scale             = 32.0                                 # scale parameter\n",
    "\n",
    "# # Train dataloader\n",
    "# max_frames_train  = 200                                  # number of frame to train\n",
    "# train_path        = '../data/voxceleb1_dev/wav'          # path to train wav files\n",
    "# batch_size_train  = 128                                  # batch size to train\n",
    "# pin_memory        = False                                # pin memory\n",
    "# num_workers_train = 5                                    # number of workers to train\n",
    "# shuffle           = True                                 # shuffling of training examples\n",
    "\n",
    "# # Validation dataloader\n",
    "# max_frames_val    = 1000                                 # number of frame to validate\n",
    "# val_path          = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "# batch_size_val    = 128                                  # batch size to validate\n",
    "# num_workers_val   = 5                                    # number of workers to validate\n",
    "\n",
    "# # Test dataloader\n",
    "# max_frames_test   = 1000                                 # number of frame to test\n",
    "# test_path         = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "# batch_size_test   = 128                                  # batch size to test\n",
    "# num_workers_test  = 5                                    # number of workers to test\n",
    "\n",
    "# # Optimizer\n",
    "# lr                = 2.5                                  # learning rate value\n",
    "# weight_decay      = 0                                    # weight decay value\n",
    "\n",
    "# # Scheduler\n",
    "# val_interval      = 5                                    # frequency of validation step\n",
    "# max_epoch         = 40                                   # number of epoches\n",
    "\n",
    "# # Augmentation\n",
    "# musan_path        = '../data/musan_split'                # path to splitted SLR17 dataset\n",
    "# rir_path          = '../data/RIRS_NOISES/simulated_rirs' # path to SLR28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "773bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data lists\n",
    "import numpy as np \n",
    "train_list = []\n",
    "val_list   = []\n",
    "test_list  = []\n",
    "\n",
    "with open('/mnt/storage/work_dir/databases/voxceleb1_test/iden_split.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "black_list = os.listdir('/mnt/storage/work_dir/databases/voxceleb1_test/')   # exclude speaker IDs from VoxCeleb1 test set\n",
    "num_train_spk = []                                      # number of train speakers\n",
    "\n",
    "# with open('../data/voxceleb1_test/iden_split.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# black_list = os.listdir('../data/voxceleb1_test/wav')   # exclude speaker IDs from VoxCeleb1 test set\n",
    "# num_train_spk = []                                      # number of train speakers\n",
    "\n",
    "\n",
    "train_sp_id = []\n",
    "for line in lines:\n",
    "    line   = line.strip().split(' ')\n",
    "    spk_id = line[1].split('/')[0]\n",
    "    \n",
    "    if not (spk_id in black_list):\n",
    "        num_train_spk.append(spk_id)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Train list\n",
    "    if (line[0] == '1'):\n",
    "        train_sp_id.append(spk_id)\n",
    "        train_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Validation list\n",
    "    elif (line[0] == '2'):\n",
    "        val_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Test list\n",
    "    elif (line[0] == '3'):\n",
    "        test_list.append(' '.join([spk_id, line[1]]))\n",
    "        \n",
    "num_train_spk = len(set(num_train_spk))\n",
    "train_sp_id = np.array(train_sp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94101e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d872a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e497662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder ASP.\n",
      "Initialised Prototypical Loss\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "# model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "# # trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "# trainfunc = AnglProto()\n",
    "# main_model = MainModel(model, trainfunc).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec6d5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "from DatasetLoader import train_dataset_sampler\n",
    "\n",
    "\n",
    "train_dataset = train_dataset_loader(train_list=train_list, max_frames=max_frames_train, train_path=train_path)\n",
    "\n",
    "train_sampler = train_dataset_sampler(train_dataset, 2, 500, batch_size_train, False, 10)\n",
    "\n",
    "train_sampler.__iter__()\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=False, sampler=train_sampler)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5358d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised SGD optimizer.\n",
      "Initialised OneCycle LR scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "# optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# scheduler = OneCycleLRScheduler(optimizer, \n",
    "#                                 pct_start=0.30, \n",
    "#                                 cycle_momentum=False, \n",
    "#                                 max_lr=lr, \n",
    "#                                 div_factor=20, \n",
    "#                                 final_div_factor=10000, \n",
    "#                                 total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73210fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b26af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef333a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n",
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n",
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n",
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n",
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n",
      "torch.Size([128, 32240])\n",
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11574/1916708691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_top1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_top1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/sr_labs_book/lab3/exercises_blank.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(train_loader, main_model, optimizer, scheduler, num_epoch, verbose, device_id)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdata_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mnloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mnloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/sr_labs_book/lab3/exercises_blank.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, label)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__S__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/sr_labs_book/lab3/exercises_blank.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/sr_labs_book/lab3/ResNetBlocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start_epoch = 0\n",
    "# checkpoint_flag = False\n",
    "\n",
    "# if checkpoint_flag:\n",
    "#     start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0004.pth')\n",
    "#     start_epoch = start_epoch + 1\n",
    "\n",
    "# # Train model\n",
    "# for num_epoch in range(start_epoch, max_epoch):\n",
    "#     train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    \n",
    "#     print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "#     if (num_epoch + 1)%val_interval == 0:\n",
    "#         _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "#         print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "#         saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models_long')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9d796",
   "metadata": {},
   "source": [
    "**3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных**\n",
    "\n",
    "Известно, что рроцедуры формирования и передачи речевого сигнала могут сопровождаться воздействием шумов и помех, приводящих к искажению сигнала. В качестве примеров искажающих факторов, влияющих на ухудшение качестве речевого сигнала можно привести: импульсный отклик помещения (реверберация), фоновый шум голосов группы нецелевых дикторов, звук телевизора или радиоприёмника и т.п. Разработка конвейера системы голосовой биометрии требует учёта воздействия искажающих факторов на качество её работы. Поскольку процедура построения современных дикторских моделей основана на обучении глубоких нейронных сетей, требующих большие объёмы данных для обучения их параметров, возможным вариантом увеличения тренировочной выборки может являться использование методов аугментации статистических данных. *Аугментация* – методика создания дополнительных обучающих примеров из имеющихся данных путём внесения в них искажений, которые могут потенциально возникнуть на этапе итогового тестирования системы.\n",
    "\n",
    "Как правило, при решении задачи аугментации данных в речевой обработке используются дополнительные базы шумов и помех. В качестве примеров можно привести базы [SLR17](https://openslr.org/17/) (корпус музыкальных, речевых и шумовых звукозаписей) и [SLR28](https://openslr.org/28/) (база данных реальных и симулированных импульсных откликов комнат, а также изотропных и точечных шумов). Важно отметить, что перед применением с использованием методов аугментации подобных баз к имеющимся данным, требуется убедиться, что частоты дискретизации искажающих баз и оригинальных данных являются одинаковыми. Применительно к рассматриваемому лабораторному практикуму частоты дискретизации всех используемых звукозаписей должны быть равными 16кГц.\n",
    "\n",
    "Как известно, можно выделить два режима аугментации данных: *онлайн* (применяется в ходе процедуры обучения) и *оффлайн* (применяется до процедуры обучения) аугментацию. В рамках настоящей лабораторной работы предлагается использовать онлайн аугментацию в силу не очень большого набора тренировочных данных и большей гибкости экспериментов, чем вс случае онлайн аугментации. Необходимо отметить, что применение онлайн аугментации на практике замедляет процедуру обучения, по сравнению с оффлайн аугментацией, так как наложение искажений, извлечение акустических признаков и их возможная предобработка требует определённого машинного времени.\n",
    "\n",
    "В рамках настоящего пункта предлагается сделать следующее:\n",
    "\n",
    "1. Загрузить и извлечь данные из базы SLR17 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию. Поскольку звукозаписи рассматриваемой базы являются достаточно длинными, рекомендуется предварительно разбить эту базу на более маленькие фрагменты (например, длительностью 5 секунд с шагом 3 секунды), сохранив их на диск. \n",
    "\n",
    "2. Загрузить и извлечь данные из базы SLR28 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию.\n",
    "\n",
    "3. Модернизировать загрузчик тренировочных данных под возможность случайного наложения (искажаем исходные звукозаписи) и не наложения (не искажаем исходные звукозаписи) одного из четырёх типов искажений (реверберация, музыкальный шум, фоновый шум голосов нескольких дикторов, неструктурированный шум), описанных внутри класса **AugmentWAV** следующего программного кода: **../common/DatasetLoader.py**.\n",
    "\n",
    "4. Используя процедуру обучения из предыдущего пункта с идентичными настройками выполнить тренировку параметров блока генерации дикторских моделей на исходных данных при наличии их аугментирвоанных копий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Download SLR17 (MUSAN) and SLR28 (RIR noises) datasets\n",
    "# with open('../data/lists/augment_datasets.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# download_dataset(lines, user=None, password=None, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f83ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract SLR17 (MUSAN)\n",
    "# extract_dataset(save_path='../data', fname='../data/musan.tar.gz')\n",
    "\n",
    "# # Extract SLR28 (RIR noises)\n",
    "# part_extract(save_path='../data', fname='../data/rirs_noises.zip', target=['RIRS_NOISES/simulated_rirs/mediumroom', 'RIRS_NOISES/simulated_rirs/smallroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a403f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split MUSAN (SLR17) dataset for faster random access\n",
    "# split_musan(save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27149f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c002fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc = AnglProto()\n",
    "main_model = MainModel(model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, \n",
    "                                     max_frames=max_frames_train, \n",
    "                                     train_path=train_path, \n",
    "                                     augment=True, \n",
    "                                     musan_path=musan_path, \n",
    "                                     rir_path=rir_path)\n",
    "\n",
    "\n",
    "train_sampler = train_dataset_sampler(train_dataset, 2, 500, batch_size_train, False, 10)\n",
    "\n",
    "train_sampler.__iter__()\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=shuffle, sampler=train_sampler)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e990b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.30, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=20, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "import time\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "\n",
    "main_model = main_model.load_state_dict(torch.load(\"8.pt\"))\n",
    "# Train model\n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    start = time.time()\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    torch.save(main_model.state_dict(), str(num_epoch) + \".pt\")\n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models_aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f235a",
   "metadata": {},
   "source": [
    "**4. Тестирование блока построения дикторских моделей**\n",
    "\n",
    "Из литературы известно, что применение алгоритмов машинного обучения на практике требует использования трёх наборов данных: *тренировочное множество* (используется для обучения параметров модели), *валидационное множество* (используется для настройки гиперпараметров), *тестовое множество* (используется для итогового тестирования).\n",
    "\n",
    "В рамках настоящего пункта предлагается выполнить итоговое тестирования блоков генерации дикторских моделей, обученных без аугментации и с аугментацией тренировочных данных, и сравнить полученные результаты. При проведении процедуры тестирования рекомендуется выбрать различное количество фреймов для тестовых звукозаписей, чтобы грубо понять то, как длительность фонограммы влияет на качество распознавания диктора.\n",
    "\n",
    "В качестве целевой метрики предлагается использовать *долю правильных ответов*, то есть количество верно классифицированных объектов по отношению к общему количеству объектов тестового множества. Как и при проведении процедуры обучения и валидации, рассматриваемая процедура тестирования предполагает решение задачи идентификации диктора на закрытом множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test dataloader\n",
    "test_dataset = test_dataset_loader(test_list=test_list, max_frames=max_frames_test, test_path=test_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, num_workers=num_workers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model without augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752082b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4189778",
   "metadata": {},
   "source": [
    "**5. Контрольные вопросы**\n",
    "\n",
    "1. Что такое верификация и идентицикация диктора?\n",
    "\n",
    "2. Что такое распознавание диктора на закрытом и открытом множестве?\n",
    "\n",
    "3. Что такое текстозависимое и текстонезависимое распознавание диктора?\n",
    "\n",
    "4. Описать схему обучения блока генерации дикторских моделей на основе нейронных сетей.\n",
    "\n",
    "5. Описать основные компоненты, из которых состоит нейросетевой блок генерации дикторских моделей (фреймовый уровень, слой статистического пулинга, сегментный уровень, выходной слой).\n",
    "\n",
    "6. Как устроены нейросетевые архитектуры на основе ResNet-блоков?\n",
    "\n",
    "7. Что такое полносвязная нейронная сеть прямого распространения?\n",
    "\n",
    "8. Как устроена стоимостная функция для обучения нейросетевого блока генерации дикторских моделей?\n",
    "\n",
    "9. Что такое аугментация данных?\n",
    "\n",
    "10. Что такое дикторский эмбеддинг и на каком уровне блока построения дикторских моделей он генерируется?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d110ea",
   "metadata": {},
   "source": [
    "**6. Список литературы**\n",
    "\n",
    "1. Bai Z., Zhang X.-L., Chen J. Speaker recognition based on deep learning: an overview // \tarXiv:2012.00931 [eess.AS] ([ссылка](https://arxiv.org/pdf/2012.00931.pdf)).\n",
    "\n",
    "2. Hansen J.H.L., Hasan T. Speaker recognition by machines and humans: a tutorial review // IEEE Signal Processing Magazine, 2015. V. 32. № 6. P. 74–99 ([ссылка](https://www.researchgate.net/publication/282940395_Speaker_Recognition_by_Machines_and_Humans_A_tutorial_review))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
